{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Gradient Descent minimizes the cost function iteratively\n",
    " To implement, we need to implement the graident of the cost function with regards to each model parameter $\\theta_j$\n",
    " Essentially we need to calculate how much the cost function changes for a small change in $\\theta_j$ (the _partial derivative_)\n",
    "The partial derivative is noted as: ${\\delta\\over{\\delta\\theta_j}} MSE(\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial derivatives of the cost function for parameter $\\theta$\n",
    "##### ${{\\partial\\over{\\partial\\theta_j}}MSE(\\theta)} = {{2 \\over m} \\sum^m_{i=1}{\\biggr( \\theta^T . \\mathbf x^{(i)} -y^{(i)} \\biggr) x^{(i)}_j} }$ \n",
    "\n",
    "Notation\n",
    "* $m$ = Number of instances in the dataset\n",
    "* $x^{(i)}$ = vector of all feature values excluding label of the $i^{th}$ instance\n",
    "* $y^{(i)}$ = is the label for the $i^{th}$ instance\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "Rather than compute gradients individually we can copute all of them for the dataset in one go. We will effectively be getting a gradient vector $\\nabla_{\\theta}MSE(\\theta)$ which will contain all partial derivatives of the cost function for each model parameter.\n",
    "\n",
    "Over a full training set the formula would be:\n",
    "\n",
    "${\\nabla_{\\theta}MSE(\\theta)}={\\left(\n",
    "                                  \\begin{array}{c} \n",
    "                                    {{\\partial\\over{\\partial\\theta_0}}MSE(\\theta)}\\\\\n",
    "                                    {{\\partial\\over{\\partial\\theta_1}}MSE(\\theta)}\\\\\n",
    "                                    {{\\partial\\over{\\partial\\theta_2}}MSE(\\theta)}\\\\\n",
    "                                    {\\vdots}\\\\\n",
    "                                    {{\\partial\\over{\\partial\\theta_n}}MSE(\\theta)}\\\\\n",
    "                                  \\end{array} \n",
    "                                \\right)} = {2\\over m}{\\mathbf X^T . (\\mathbf X. \\theta - \\mathbf y)}$\n",
    "\n",
    "the Gradient vector points uphill, to go downhill (the descent) we subtract ${\\nabla_{\\theta}MSE(\\theta)}$ from $\\theta$ . To determine the size of the descent multiply the gradient vector by learning rate - $\\eta$ \n",
    "Represented by the equation\n",
    "###  ${\\theta^{(next step)} =  \\theta - \\eta {\\nabla_{\\theta}MSE(\\theta)}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple Gradient Descent implementation\n",
    "\n",
    "#Prep stuff\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "#Generate random linear looking data\n",
    "X = 2 * rnd.rand(100, 1)\n",
    "y = 4 + 3 * X + rnd.randn(100, 1)\n",
    "\n",
    "import numpy.linalg as LA\n",
    "x = np.ones((100, 1)) \n",
    "X_b = np.c_[x, X] # concatenate our features to array vector of 1's\n",
    "#theta_best = LA.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eta = 0.1\n",
    "iterations = 1000\n",
    "m = 100\n",
    "\n",
    "# random initialization\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta - eta * gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.20823178],\n",
       "       [ 2.75650503]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gradients returns the same result as the Normal Equation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
