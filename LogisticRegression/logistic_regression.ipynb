{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Logistic Regression\n",
    "Also known as Logit Regression\n",
    "Generally used to estimate the probability an instance belong to a particular class. If it is more than $50%$ the model predicts positively it belongs to that class and lableled 1. If it predicts less than $50%$ it is labelled 0. Effectively a binary classifier.\n",
    "Say, what is probability of an email being spam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How does it work.\n",
    "A logistic model computes the weighted sum of it's input features (with a bias term) and return the **_logistic_** of the output.\n",
    "\n",
    "**Note** Logistic regression is similar to Linear Regression without the **_logit_** step.\n",
    "\n",
    "Noted vectorized form: $\\hat p = h_\\theta (\\mathbf x ) = \\sigma \\bigr( \\theta^T . \\mathbf X \\bigr)$\n",
    "\n",
    "The logistic (logit) noted as $\\sigma(.)$ is a **_sigmoid function_** (s-shaped) \n",
    "\n",
    "noted as: $\\sigma(t) = {1 \\over {1 + exp(-t)}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training \n",
    "\n",
    "The goal with training is to set the parameter vector **$\\theta$** that enables the model to testimate **high** probability of positive instances **$(y = 1)$** and **low** probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
