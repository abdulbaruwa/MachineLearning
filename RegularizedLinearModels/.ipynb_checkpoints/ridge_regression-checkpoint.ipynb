{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to deal with overfitting is to constrain the model. A process known as Regularization. Effectively we reduce the reach of the model making it harder to overfit. \n",
    "When the model is linear we can achieve this by constraining the weights of the model.\n",
    "If the model is polynomial we can also constrain it by reducing the number of polynomial degrees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "This is basically a regularized version of Linear Regression.\n",
    "How:\n",
    "During training (and only during) a regularization term ( $\\propto \\sum^n_{i=1} \\theta^2_i $) is added to the models  cost function.\n",
    "Applying the term forces the learning algorithm to fit the data and also keep the model weights small.\n",
    "After training use uregularized perf measure.\n",
    "The cost function with regression can be defined as $J(\\theta) = MSE(\\theta) + \\propto {1 \\over 2} \\sum^n_{i=1}\\theta^2_i$\n",
    "\n",
    "* Ridge Regression is sensitive to the scale of input features. Remember to scale the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression can be performed\n",
    "* Computing the close-form equation which can be represented as:  $\\hat\\theta = {\\biggr(\\mathbf  X^T \\mathbf{.}\\mathbf{X} + \\propto\\mathbf{A} \\biggr)}^{-1} . \\mathbf{X} . \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.39804895]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
    "m = 20\n",
    "X = 3 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "\n",
    "\n",
    "rregularizer = Ridge(alpha=1, solver=\"cholesky\") # setting the solver hyperparam - we are using Andre Cholesky's matrix factoriaztion technique\n",
    "rregularizer.fit(X, y)\n",
    "rregularizer.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute using Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07209499])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_regularizer = SGDRegressor(penalty='l2', random_state=42)\n",
    "sgd_regularizer.fit(X, y.ravel())\n",
    "sgd_regularizer.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
