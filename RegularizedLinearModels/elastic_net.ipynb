{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net\n",
    "This is a kind of half-way house between Ridge Regression and Lasso Regression (Least Absolute Shrink & Selection operator regression).\n",
    "In which the Regularization term is a mix of Ridge and Lasso regularization terms, with some control voer the mix ratio 'r'.\n",
    "* $ r == 0$ Elastic net is effectively Ridge Regression\n",
    "* $ r == 1$ Elastic net is effectively Lasso Regression\n",
    "\n",
    "Noted as \n",
    "$$ (\\theta) = MSE(\\theta) + {r\\alpha} {\\sum^n_{i=1}} {\\bigr| \\theta_i\\bigr|} + {{1-r}\\over 2}\\alpha {\\sum^n_{i=1}} \\theta^2_i $$\n",
    "\n",
    "#### Which to use, Linear Regression, Ridge Regression, Lasso or Elastic Net:\n",
    "Avoid using plain linear regression. \n",
    "* Good default is Ridge Regression\n",
    "* Use Lasso or Elastic Net where the number of useful features are less. Remember these reduce weights of features that are not useful to zero.\n",
    "* Where feature numbers is greater than training instances - Use Elastic Net. (Lasso will be erratic in such a scenario).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Early Stopping\n",
    "A way of regularizing learning algorithms like Gradient Descent.\n",
    "\n",
    "Where training is stopped as soon as the validation error reaches a minimum.\n",
    "Example:\n",
    "Say we are training a model with Batch Gradient Descent. As learning over epochs occurs.\n",
    "The prediction error (RMSE in this case) on the training set will expectedly go down and so does the prediction on the validation set.\n",
    "However, after a while the validation error stops decreasing and at some point starts increasing. Indication the model is overfitting the training data. At this point we stop training.\n",
    "\n",
    "#### This is a very efficient and simple regularization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# standardize notebook result over diff runs\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data setup\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n",
    "polynomial_scaler = Pipeline((\n",
    "('poly_features', PolynomialFeatures(degree=90, include_bias=False)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "X_train_pscaled = polynomial_scaler.fit_transform(X_train)\n",
    "X_val_pscaled = polynomial_scaler.transform(X_val)\n",
    "\n",
    "sgd_regressor = SGDRegressor(n_iter=1, penalty=None, eta0=0.0005,warm_start=True, learning_rate='constant', random_state=42)\n",
    "epochs = 50\n",
    "\n",
    "training_errors, validation_errors = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sgd_regressor.fit(X_train_pscaled, y_train)\n",
    "    y_train_predict = sgd_regressor.predict(X_train_pscaled)\n",
    "    y_validation_predict = sgd_regressor.predict(X_val_pscaled)\n",
    "    training_errors.append(mean_squared_error(y_train_predict, y_train))\n",
    "    validation_errors.append(mean_squared_error(y_validation_predict, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
