{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to deal with overfitting is to constain the model. A process is known as Regularization. Effectively we reduce the reach of the model making it harder to overfit. \n",
    "When the model is linear we can achieve this by constraining the weights of the model.\n",
    "If the model is polynomial we can also constrain it by reducing the number of polynomial degrees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "This is basically a regularized version of Linear Regression.\n",
    "How:\n",
    "During training (and only during) a regularization term ( $\\propto \\sum^n_{i=1} \\theta^2_i $) is added to the models  cost function.\n",
    "Applying the term forces the learning algorithm to fit the data and also keep the model weights small.\n",
    "After training use uregularized perf measure.\n",
    "The cost function with regression can be defined as $J(\\theta) = MSE(\\theta) + \\propto {1 \\over 2} \\sum^n_{i=1}\\theta^2_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
